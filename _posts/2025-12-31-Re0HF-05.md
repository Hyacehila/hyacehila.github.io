---
layout: blog-post
title: "Re0-05 : TRL GRPOTrainer（实战篇）"
date: 2025-12-31 22:00:00 +0800
series: "Re0 : 从0开始的LLM Finetuning"
categories: [Finetuning]
tags: [RL, GRPO]
author: Hyacehila
excerpt: GRPO 实战：GRPOTrainer、监控调试与完整示例
---

# 第五章：使用 GRPO 进行可验证奖励训练（实战篇）

> 配套代码：`grpotrainer.py`

本篇是 GRPO 的实战指南。我们将使用 TRL 库的 `GRPOTrainer`，在一个具体的数学任务上跑通从数据准备、奖励函数编写到训练监控的全流程。关于 GRPO 的算法原理，请回顾[上一章（原理篇）]({% post_url 2025-12-30-Re0HF-04 %})。

## 1. GRPOTrainer 的工程实现

理解 `GRPOTrainer` 的内部运作方式，对于调节显存和训练速度至关重要。与 SFT 的“一次前向传播”不同，GRPO 的单步训练是一个多阶段过程：

1.  **Online Sampling (在线采样)**：
    对于每个 Prompt，策略模型会实时生成 $G$ 个不同的回答（由 `num_generations` 参数控制）。
    *   *显存影响*：这一步需要存储生成的 Token，显存占用随 $G$ 和序列长度线性增加。
    *   *速度影响*：这是训练中最耗时的步骤，通常占据 70% 以上的时间。

2.  **Reward Evaluation (奖励评估)**：
    将这 $G$ 个回答送入我们定义的 `reward_function`。如果是简单的规则匹配（如正则匹配答案），这步极快；如果是调用外部模型作为奖励模型，这步也会成为瓶颈。

3.  **Advantage Calculation (优势计算)**：
    基于我们在原理篇推导的公式，计算这 $G$ 个回答的相对优势：
    $$ A_i = \frac{r_i - \mu}{\sigma} $$
    其中 $\mu$ 和 $\sigma$ 分别是该组 $G$ 个奖励的均值和标准差。这一步将绝对奖励（如 0 或 1）转化为相对信号（如 -1.5 或 +1.5），从而消除不同 Prompts 之间的难度差异。

4.  **Policy Update (策略更新)**：
    利用计算出的优势，对策略模型进行反向传播更新。同时计算与参考模型的 KL 散度作为正则项。

## 2. 关键参数配置指南

在使用 `GRPOConfig` 时，有三个参数决定了训练的成败：

### num_generations (G)
这是 GRPO 的灵魂参数。它决定了“群组”的大小。
*   **推荐值**：`4` 到 `16`。
*   **权衡**：$G$ 越大，优势估计（Baseline）越准确（即 $\mu$ 和 $\sigma$ 越接近真实分布），训练越稳定，但显存开销和时间成本也成倍增加。
*   **注意**：`per_device_train_batch_size` 在 GRPO 中指的是 **Prompt 的数量**。实际送入模型的 Batch Size 将是 `batch_size * num_generations`。务必注意显存爆炸！

### temperature
控制采样多样性的参数。
*   **推荐值**：`0.7` 到 `1.0`。
*   **逻辑**：GRPO 依赖于模型生成“不同”的回答来进行比较。如果温度太低（接近 0），模型生成的 $G$ 个回答可能完全一样，导致 $\sigma \approx 0$，优势计算失效（$A_i$ 出现除零错误或无梯度），训练将无法进行。

### kl_coef (Beta)
KL 惩罚项的系数。
*   **推荐值**：`0.01` 到 `0.1`。
*   **作用**：防止模型为了刷高分而通过“作弊”的方式（如输出乱码欺骗正则匹配）偏离语言模型的正常分布。

## 3. 实战示例：GSM8K 数学任务

我们将训练一个模型，使其学会按照标准格式（在最后输出 `#### 答案`）解决数学问题。

### 3.1 数据准备

GRPO 只需要 Prompt，不需要标准的 Target。

```python
from datasets import load_dataset

def format_gsm8k(sample):
    # 构造 Prompt，不需要包含答案
    return {
        "prompt": f"Question: {sample['question']}\n\nPlease reason step by step, and put your final answer within ####.\n\nAnswer:"
    }

dataset = load_dataset("gsm8k", "main", split="train")
dataset = dataset.map(format_gsm8k)
```

### 3.2 自定义奖励函数

这是 GRPO 最灵活的部分。我们将定义两个奖励：一个是**格式奖励**（是否按要求输出），一个是**正确性奖励**（答案数值是否对）。

```python
import re

def reward_correctness(prompts, completions, **kwargs):
    """验证答案数值是否正确"""
    rewards = []
    for completion, target in zip(completions, kwargs["answer"]): # answer 来自数据集的原始列
        # 提取模型输出的数值
        match = re.search(r"####\s*(\-?[0-9\.]+)", completion)
        if match:
            pred = float(match.group(1))
            # 提取标准答案数值
            gold = float(target.split("####")[-1].strip().replace(",", ""))
            rewards.append(1.0 if abs(pred - gold) < 1e-4 else 0.0)
        else:
            rewards.append(0.0)
    return rewards

def reward_format(prompts, completions, **kwargs):
    """验证是否包含特定格式标记"""
    rewards = []
    for completion in completions:
        # 只要输出了 #### 就给一点辛苦分，鼓励模型遵循格式
        if "####" in completion:
            rewards.append(0.1)
        else:
            rewards.append(0.0)
    return rewards
```

### 3.3 启动训练

```python
from trl import GRPOTrainer, GRPOConfig

training_args = GRPOConfig(
    output_dir="grpo_gsm8k",
    num_generations=4,          # 每组生成 4 个样本
    max_prompt_length=256,
    max_completion_length=256,  # 留足推理空间
    per_device_train_batch_size=1, # 实际 batch = 1 * 4 = 4
    learning_rate=1e-6,         # RL 训练通常需要较低的学习率
    bf16=True
)

trainer = GRPOTrainer(
    model=model,
    reward_funcs=[reward_correctness, reward_format], # 支持多个奖励函数自动求和
    args=training_args,
    train_dataset=dataset,
    peft_config=lora_config,
)

trainer.train()
```

## 4. 训练监控：读懂 Loss 曲线

在 GRPO 训练中，Loss 下降并不总是意味着好事，我们需要重点关注 `rewards/mean`。

*   **阶段一：探索期**
    `rewards/mean` 极低且震荡。此时模型正在尝试随机生成，偶尔碰对格式获得一点格式奖励。
    
*   **阶段二：爬坡期**
    `rewards/mean` 快速上升。模型掌握了格式（`####`），开始尝试推理数值。此时优势函数开始发挥作用，正确的推理路径被强化。
    
*   **阶段三：收敛期**
    `rewards/mean` 趋于平稳。

**调试技巧**：如果 `objective/kl` 突然爆炸（>10），说明模型“玩坏了”（Reward Hacking），正在生成高分但不可读的文本。此时应增大 `kl_coef` 或减小学习率。

## 系列总结与展望

至此，"Re0 : 从0开始的LLM Finetuning" 系列暂告一段落。我们从最基础的 **Trainer** (SFT) 出发，进阶到自动化的 **SFTTrainer**，再到处理偏好的 **DPOTrainer**，最后到达了强化学习的前沿 **GRPOTrainer**。

这一路走来，我们见证了微调范式的演变：
1.  **SFT**：学会说话（模仿）。
2.  **DPO**：学会分辨好坏（对齐）。
3.  **GRPO**：学会自我探索与进化（推理）。

但这并不是终点。随着 **DeepSeek-R1** 等模型的发布，**Test-Time Compute (推理时计算)** 和 **Self-Play (自博弈)** 正在成为新的热点。未来的微调将不再局限于权重更新，而是更多地通过 System 2 的思考过程来提升能力。

也许不久后，我们会在新的系列中讨论 **Verl** 等新一代强化学习框架，探索如何让模型在推理阶段“慢下来，想清楚”。

感谢阅读，Happy Finetuning!
