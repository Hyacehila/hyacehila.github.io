---
layout: blog-post
title: "Re0-04 : TRL GRPOTrainer（原理篇）"
date: 2025-12-30 22:00:00 +0800
series: "Re0 : 从0开始的LLM Finetuning"
categories: [Finetuning]
tags: [RL, GRPO]
author: Hyacehila
excerpt: GRPO 原理：在线采样、群组相对优势与奖励系统
---

# 第四章：使用 GRPO 进行可验证奖励训练（原理篇）

> 配套代码：`grpotrainer.py`

## 前言：从主观偏好到客观真值

在上一章中，我们通过 DPO 解决了“偏好对齐”问题。DPO 非常适合那些没有标准答案、依赖人类主观判断的任务（如“写一首更优雅的诗”）。然而，当我们面对**数学推理**、**代码生成**等有明确正误标准的任务时，DPO 的静态数据训练方式显得效率低下。

**GRPO (Group Relative Policy Optimization)** 是一种专门为**可验证奖励（Verifiable Reward）** 场景设计的在线强化学习算法。它通过在线采样和群组竞争，让模型在不断的试错中自我进化。本章将深入解析 GRPO 的数学原理，并对比其与传统 PPO 算法的异同。

## 1. GRPO 的理论推导

GRPO 的核心创新在于移除了 PPO 中的 Critic（价值网络），改用群组内的相对优势来引导策略更新。

### 1.1 强化学习的基本目标

在 RLHF 中，我们的标准目标是最大化期望奖励，同时约束策略模型不偏离参考模型：

$$
J(\theta) = \mathbb{E}_{q \sim P(Q), o \sim \pi_\theta(o|q)} [r(q, o)] - \beta \mathbb{D}_{KL}(\pi_\theta || \pi_{\text{ref}})
$$

其中 $q$ 是 Prompt，$o$ 是模型生成的输出，$r(q, o)$ 是奖励函数。

### 1.2 PPO 的优化范式

为了优化上述目标，传统的 PPO (Proximal Policy Optimization) 算法使用了基于 Critic 的优势估计。其核心的损失函数（CLIP Loss）如下：

$$
\mathcal{L}_{\text{PPO}}(\theta) = -\mathbb{E}_t \left[ \min \left( \frac{\pi_\theta(o_t|q)}{\pi_{\theta_{old}}(o_t|q)} A_t, \text{clip}\left(\frac{\pi_\theta(o_t|q)}{\pi_{\theta_{old}}(o_t|q)}, 1-\epsilon, 1+\epsilon\right) A_t \right) \right]
$$

其中优势函数 $A_t$ 通常通过 **GAE (Generalized Advantage Estimation)** 计算，这依赖于一个额外训练的价值网络 $V_\phi(s)$：
$$ A_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \dots $$
$$ \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t) $$

这种方法的缺点显而易见：
1.  **显存开销大**：需要加载策略模型 $\pi_\theta$ 和价值网络 $V_\phi$（通常与策略模型一样大）。
2.  **训练不稳定**：价值网络的估计误差会传播给策略网络，导致训练震荡。

### 1.3 GRPO 的目标函数

GRPO 摒弃了价值网络，提出了一种基于**群组归一化**的优势估计方法。其目标函数如下：

$$
J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{ \min \left( \frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,<t})} A_{i,t}, \text{clip}(\dots) A_{i,t} \right) - \beta \mathbb{D}_{KL}(\pi_\theta || \pi_{\text{ref}}) \right\} \right]
$$

公式解析：
1.  **采样**：对于每个 Prompt $q$，从旧策略 $\pi_{\theta_{\text{old}}}$ 中采样一组输出 $\{o_1, o_2, ..., o_G\}$。
2.  **优势计算**：对于第 $i$ 个输出，其优势 $A_i$ 不再依赖 Critic，而是基于组内其他样本的相对表现计算：
    $$ A_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\})} $$
3.  **策略更新**：如果某个输出的奖励 $r_i$ 高于组内平均值，则 $A_i > 0$，模型会增加生成该输出的概率；反之则减少。

### 1.4 为什么 GRPO 更高效？

通过上述公式对比，我们可以看到 GRPO 的核心优势：

*   **无需 Critic**：优势 $A_i$ 直接通过组内统计量（Mean, Std）计算得出。这使得显存占用减半，且消除了 Critic 训练不收敛带来的风险。
*   **自适应基准**：基准（Baseline）是动态变化的组内平均值 $\text{mean}(r)$。这天然适应了不同难度的问题（有的问题大家都只能得 0.1 分，有的都能得 0.9 分，相对优势依然有效）。

## 2. 奖励系统设计

在 GRPO 中，奖励函数 $r(q, o)$ 是训练的指挥棒。根据反馈来源，奖励可以分为两类：

### 2.1 结果监督 (Outcome Supervision, OS)

这是最直接的奖励方式，仅关注最终结果的正确性。

*   **场景**：数学题答案匹配、LeetCode 单元测试通过率。
*   **形式**：
    $$ r(q, o) = \mathbb{I}(\text{Check}(o) == \text{Pass}) $$
*   **优点**：信号极其准确，不存在歧义。
*   **缺点**：信号稀疏。对于复杂问题，模型很难偶然生成完全正确的答案，导致长时间无法获得正向奖励（Cold Start 问题）。

### 2.2 过程监督 (Process Supervision, PS)

为了解决结果监督的稀疏性，我们可以引入过程奖励。

*   **场景**：分步数学推理、代码逻辑检查。
*   **形式**：
    $$ r(q, o) = w_1 \cdot r_{\text{format}} + w_2 \cdot r_{\text{step}} + w_3 \cdot r_{\text{outcome}} $$
    例如：
    *   **格式奖励**：使用了 `<think>` 标签加 0.1 分。
    *   **步骤奖励**：每推理正确一步加 0.2 分。
    *   **结果奖励**：最终答案正确加 1.0 分。
*   **优点**：信号密集，能引导模型学习解题的“规范”和“思路”。

## 3. 强化学习的统一范式

如果我们从更高的视角审视 SFT、DPO 和 GRPO，会发现它们只是同一通用梯度更新公式的不同变体：

$$ \nabla J(\theta) = \mathbb{E}_{q, o \sim \pi} \left[ \text{Weight}(q, o) \cdot \nabla \log \pi_\theta(o|q) \right] $$

| 方法 | 权重 (Weight) | 物理含义 |
| :--- | :--- | :--- |
| **SFT** | $1$ | **模仿**：所有训练数据都是“对”的，无脑增加概率。 |
| **DPO** | $\sigma(r_w - r_l)$ 相关 | **比较**：增加优于参考模型的回答的概率。 |
| **GRPO** | $A_i$ (相对优势) | **探索**：增加优于当前平均水平的尝试的概率。 |

这种从**离线模仿**（SFT/DPO）到**在线探索**（GRPO）的演进，标志着模型从“背诵知识”向“习得能力”的跨越。

---

## 参考资料

- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Shao et al., 2024)](https://arxiv.org/abs/2402.03300)
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1)
- [Hugging Face TRL: GRPOTrainer Documentation](https://huggingface.co/docs/trl/main/en/grpo_trainer)
