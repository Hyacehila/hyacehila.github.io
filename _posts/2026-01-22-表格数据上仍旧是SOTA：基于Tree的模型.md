---
layout: blog-post
title: 表格数据上仍旧是SOTA：基于Tree的模型
date: 2026-01-22 10:00:00 +0800
categories: [机器学习]
tags: [集成学习, XGBoost,Random Forest]
author: Hyacehila
excerpt: 介绍表格数据场景下常见的树模型与梯度提升框架：从 Random Forest、GBDT 到 XGBoost/LightGBM/CatBoost，并梳理它们的优缺点与适用场景。
---

# 表格数据上仍旧是SOTA：XGBoost与基于Tree的模型

虽然深度学习已经在传统的回归与分类预测领域得到了广泛应用，但经典的统计机器学习方法在**结构化表格数据**上仍然非常强势。以树模型为核心的方案通常具备：

- 对数据量的需求更小
- 更容易通过正则化与参数约束抑制小样本过拟合
- 生态成熟，便于接入解释性工具（如 SHAP）
- 计算成本显著低于神经网络模型（很多任务可在秒级/分钟级完成训练与调参）
- 超参数数量相对可控，调参路径清晰
- 在结构化表格数据上，很多场景依然是 SOTA 或强基线

下面按方法演进，快速梳理几类常用的树模型。

## Bagging 的巅峰：Random Forest

Random Forest 在 Bagging 集成学习领域的重要性无需多说：它通过对多棵决策树的并行训练与投票/平均，在现代计算设备上往往只需要很短时间就能得到一个非常可靠的基线结果。Random Forest 也是几乎所有机器学习教材都会覆盖的经典模型，十分适合作为入门与强 baseline。

（后续我会单独整理一篇“集成学习”文章，系统介绍 Bagging/Boosting 的框架与直觉。）

## Boosting 的基石：GBDT

Bagging 的性能存在一定的理论上限：它主要依赖“并行 + 平均”来压低方差，但难以持续突破偏差带来的限制。Boosting 的思路因此出现：通过**逐步拟合残差/梯度**来不断提升模型。

在树模型家族中，GBDT（Gradient Boosting Decision Tree）是这一思想的代表性框架，也是后续更强工程化实现（如 XGBoost/LightGBM/CatBoost）的重要基础。虽然在很多实战场景中，原生 GBDT 已经被更成熟的库替代，但仍然很值得理解其思想与训练流程。

## 长期强基线：XGBoost

**XGBoost**（eXtreme Gradient Boosting）是一个开源的、高效、灵活、可扩展的梯度提升框架，基于 GBDT，由 **陈天奇**（Tianqi Chen）在攻读博士期间开发。

XGBoost 并非“发明新算法”，而是在 GBDT 的基础上做了**系统性的工程优化与理论改进**，使其更高效、稳定、易用。这也是为什么在工程实践中，人们常用 XGBoost 替代“自己手写的 GBDT”。更深入的细节可以参考论文 *XGBoost: A Scalable Tree Boosting System*。

XGBoost 的主要优势包括：

1. 模型稳定、泛化能力强
2. 在结构化/表格数据上长期是强基线（很多场景接近或达到 SOTA）
3. C++ 核心实现，工程优化充分，并支持 GPU 加速
4. 灵活性高：可自定义损失函数、评估指标与目标函数，覆盖分类、回归、排序、生存分析等任务
5. 鲁棒性更好：可自动处理缺失值；树的分裂机制对异常点相对不敏感；配合交叉验证与早停可有效控制过拟合
6. 解释性工具成熟：内置重要性 + 可接入 SHAP 等外部解释库

主要缺点包括：

1. 对超参数相对敏感，需要一定调参经验或自动化搜索
2. 训练速度通常慢于 LightGBM（尤其在大数据场景）
3. 不原生支持分类特征（通常需要预编码/目标编码等）
4. 主要适用于表格数据，或能通过特征工程转化为表格形式的任务

深入学习可参考官方文档：[XGBoost 文档](https://xgboost.readthedocs.io/en/stable/)

## 速度为王：LightGBM

**LightGBM**（Light Gradient Boosting Machine）由 **微软亚洲研究院（MSRA）** 于 **2017 年** 推出的高效梯度提升框架，面向**大规模数据 + 高效率训练**场景。

它的背景是：XGBoost 在大数据场景下训练偏慢、内存占用偏高。LightGBM 提出了 **基于梯度的单边采样（GOSS）** 和 **互斥特征捆绑（EFB）** 等关键技术，使训练速度显著提升、内存占用明显降低。

更多细节可参考官方文档：[LightGBM 文档](https://lightgbm.readthedocs.io/)

LightGBM 的优点：

1. 工程优化激进，在大规模数据上通常比 XGBoost 更快、内存更省
2. 原生支持分类特征（无需预先独热编码）
3. 对分布式训练与 GPU 加速支持良好
4. 同样适用于多种任务（分类/回归/排序等）

LightGBM 的缺点：

1. 默认的 leaf-wise 生长策略更容易过拟合（可通过 `max_depth`、`num_leaves`、正则化等约束；通常不建议为了“更不容易过拟合”而直接切回 level-wise）
2. 在小数据上优势可能不明显，甚至可能不如更“保守”的实现
3. 超参数空间更复杂，调参成本更高
4. EFB 在极端情况下可能引入噪声（可视需要关闭相关选项）

总体而言，LightGBM 更偏“工程化框架”：在高维稀疏特征（推荐/广告等）与大规模数据上非常强，但在小样本学术场景下未必占优。

## 分类特征友好：CatBoost

**CatBoost**（Categorical Boosting）由俄罗斯公司 **Yandex** 于 **2017 年** 开源，目标非常明确：**更稳健、更自动化地处理分类特征**。

在广告、搜索、推荐等业务中，类别特征（用户 ID、城市、设备型号等）非常常见；手动编码往往不稳定、且容易引入泄漏。CatBoost 提出 **Ordered Target Encoding（有序目标编码）** 与 **对称树结构** 等设计，在“少做特征工程”的前提下获得可靠表现。

官方文档：[CatBoost 文档](https://catboost.ai/en/docs/)

**CatBoost = Categorical + Boosting + No Feature Engineering Needed**

CatBoost 的优势：

- 原生支持分类特征（这是它的核心卖点）
- 相对不容易过拟合（尤其在类别特征较多的场景）
- 默认参数表现往往很好，偏“开箱即用”，减少繁琐的手工特征工程与调参
- 适用于多种任务，并具备较好的可解释性支持
- 支持 GPU 加速

CatBoost 的缺点：

- 有序编码与对称树带来额外计算开销，训练速度通常慢于 LightGBM（GPU 可缓解）
- 内存占用可能较高；可考虑调整 `one_hot_max_size` 等参数
- 对称树会限制表达能力：虽能抑制过拟合，但在部分任务上可能出现欠拟合
- 分布式支持相对弱（常见用法是单机 + GPU）

如果数据含大量字符串/ID 类特征，CatBoost 往往是很强的选择；但在百万级以上超大规模数据上，仍需结合速度与资源做取舍。