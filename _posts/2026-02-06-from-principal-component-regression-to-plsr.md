---
layout: blog-post
title: 从主成分回归 (PCR) 到偏最小二乘 (PLS)
date: 2026-02-06 12:00:00 +0800
categories: [Statistics]
tags: [Linear Regression]
author: Hyacehila
excerpt: 当数据存在多重共线性时，PCR 和 PLSR 都是常用的降维回归方法。本文详细推导了 PCR 与 PLSR 的数学原理，分析了 PCR “只看 X 不看 Y” 的潜在缺陷，并直观解释了 PLSR 如何通过引入因变量相关性来解决这一问题。
mathjax: true
---

> 本文核心观点和部分内容参考自谢益辉的文章：[主成分回归与偏最小二乘回归](https://yihui.org/cn/2008/09/principle-component-regression-and-partial-least-square-regression/)。

# 从主成分回归 (PCR) 到偏最小二乘 (PLS)
在多元线性回归（Multiple Linear Regression, MLR）中，最小二乘法（OLS）是我们最常用的参数估计方法。然而，当数据集中存在**多重共线性（Multicollinearity）**，或者自变量的数量多于样本量（$p > n$）时，OLS 估计量会变得极其不稳定甚至无法计算（因为设计矩阵 $X^TX$ 不可逆或接近奇异）。

为了解决这个问题，降维（Dimensionality Reduction）成为了一个核心思路。**主成分回归（PCR）**和**偏最小二乘回归（PLSR）**正是这类方法的两个代表。它们都通过构建新的“潜在变量”（Latent Variables）来代替原始变量进行回归，但在构建这些潜在变量的逻辑上，两者有着本质的区别，因此在使用上应当保持谨慎。

## 1. 主成分回归 (Principal Component Regression, PCR)

### 1.1 数学原理

PCR 的核心思想可以概括为：**先做 PCA（主成分分析），再做 OLS**。

假设我们有中心化后的自变量矩阵 $X \in \mathbb{R}^{n \times p}$ 和因变量 $Y \in \mathbb{R}^{n \times 1}$。

1.  **谱分解**：对 $X$ 进行奇异值分解（SVD）或对其协方差矩阵 $X^TX$ 进行特征分解。
    $$ X = U \Sigma V^T $$
    其中 $V$ 的列向量是载荷向量（Loadings），也就是主成分的方向。

2.  **构造主成分**：计算得分矩阵（Scores）$Z$。
    $$ Z = XV $$
    由于主成分之间是正交的，即 $Z^TZ$ 是对角矩阵，这完美解决了多重共线性的问题。

3.  **截断与回归**：通常我们只取前 $k$ 个特征值最大（解释方差最大）的主成分 $Z_k$。做 $Y$ 关于 $Z_k$ 的最小二乘回归：
    $$ \hat{Y} = Z_k \hat{\gamma} $$
    $$ \hat{\gamma} = (Z_k^T Z_k)^{-1} Z_k^T Y $$

4.  **还原参数**：将回归系数映射回原始空间：
    $$ \hat{\beta}_{PCR} = V_k \hat{\gamma} $$

### 1.2 方法论的缺陷

PCR 听起来很完美：它消除了共线性，保留了 $X$ 中的主要变异信息。但是，它存在一个逻辑上的致命伤：

> **主成分提取的过程仅仅依赖于自变量 $X$ 的协方差结构，完全无视了因变量 $Y$。**

在 PCA 中，我们选择主成分的标准是“方差最大化”。然而，**方差大并不意味着跟 $Y$ 相关**。可能存在这样一种情况：$X$ 的某个方向方差极小（因此在 PCR 中被丢弃），但该方向却包含了 $Y$ 的绝大部分信息。

正如 Ali S. Hadi 和 Robert F. Ling (1998) 在 *The American Statistician* 上指出的，如果解释变量的主成分与响应变量无关，PCR 的效果甚至可能不如直接丢弃变量。文章给出了一个例子，例子的现象就是，前p-1个PC跟因变量一点关系都没有，而最后一个PC解释了因变量所有的变异。原因在于PCA仅仅依赖于X的协方差结构，而忽略了Y的信息。

## 2. 偏最小二乘回归 (Partial Least Squares Regression, PLSR)

为了解决 PCR “只看 X 不看 Y” 的问题，偏最小二乘回归（PLSR）应运而生。它的核心思想是：**在寻找潜在变量时，既要让它尽可能解释 $X$ 的变异，又要让它尽可能解释 $Y$ 的变异**。

### 2.1 优化目标

假设我们要寻找一个权重向量 $w$（满足 $\|w\|=1$），构造潜在变量 $t = Xw$。PLSR 的目标函数是最大化 $t$ 与 $Y$ 的协方差：

$$ \max_{w} \text{Cov}(Xw, Y)^2 = \max_{w} \text{Var}(Xw) \cdot \text{Corr}(Xw, Y)^2 \cdot \text{Var}(Y) $$

对比 PCR 和 PLSR：
*   **PCR**：最大化 $\text{Var}(Xw)$。
*   **PLSR**：最大化 $\text{Var}(Xw) \times \text{Corr}(Xw, Y)^2$。（注：$\text{Var}(Y)$ 是常数）

这就很直观了：PLSR 试图寻找一个平衡点，它找出来的成分既包含 $X$ 的主要结构（方差大），又与 $Y$ 高度相关。 PLSR带来的 **最大的变化就是我们提取的成分不再是单一的研究变差，而是考虑了和因变量的相关性的成分**。

### 2.2 算法简述 (NIPALS 算法思想)

PLSR 的求解通常使用 NIPALS 算法或 SIMPLS 算法。对于单因变量 $Y$，其迭代过程大致如下：

1.  计算 $X$ 和 $Y$ 的协方差向量 $w = X^T Y$。
2.  归一化 $w \leftarrow w / \|w\|$。
3.  计算得分向量（Score Vector）$t = Xw$。
4.  计算 $Y$ 对 $t$ 的载荷 $c = Y^T t / (t^T t)$。
5.  计算 $X$ 对 $t$ 的载荷 $p = X^T t / (t^T t)$。
6.  **剥离（Deflation）**：从 $X$ 和 $Y$ 中减去该成分解释的部分：
    $$ X_{new} = X - t p^T $$
    $$ Y_{new} = Y - t c^T $$
7.  利用残差矩阵重复上述步骤，直到提取足够的成分。

最终，我们建立的模型形式为：
$$ X = TP^T + E $$
$$ Y = TQ^T + F $$
（其中 $Q$ 往往直接关联回归系数）。

### 2.3 几何解释

*   **OLS** 寻找的是在 $X$ 列空间中离 $Y$ 最近的投影。
*   **PCR** 先在这个空间里找一个方差最大的子空间，然后在子空间里找投影。
*   **PLSR** 则是旋转坐标轴，让新的轴既指向数据分布延伸最长的方向，又偏向于 $Y$ 梯度最大的方向。

## 3. 总结与比较

| 维度 | 主成分回归 (PCR) | 偏最小二乘回归 (PLSR) |
| :--- | :--- | :--- |
| **成分提取依据** | 仅依赖 $X$ 的方差 ($\text{Var}(X)$) | 考虑 $X$ 方差与 $X,Y$ 相关性 ($\text{Cov}(X,Y)$) |
| **监督学习** | 否（第一步无监督） | 是（利用了目标变量 Y） |
| **变量选择** | 实际上是一种硬截断 | 相当于一种软加权 |
| **适用场景** | 噪音主要在 $X$ 且 $Y$ 关联 $X$ 的主要变异时 | 预测为导向，$X$ 内部存在多重共线性时 |

在大多数实际应用中（尤其是化学计量学、光谱分析），PLSR 的表现往往优于或持平于 PCR。通过引入因变量的信息，PLSR 往往能用更少的成分达到相同的预测精度，从而得到一个更简约（Parsimonious）的模型。

我们建议使用PLSR来代替PCR，其核心原因很简单，前者在统计学上具备更优的性质，他同时利用了X和Y的信息，而不是只使用自变量自己的协方差结构。PCR的唯一优点就是对多重共线性有很好的处理，然而在统计学上，PCR的统计性质是不可信的。

在实际使用的时候，PCR是不容易出现前面描述的情况的（前p-1个成分与Y无关、最后一个成分相关），主成分在很多情况下（尤其是化学数据）能解释自变量的某种内在结构，这种结构往往是与因变量有关系的，但从理论上来说，PCR是没有回归的逻辑的。而PLSR包含了PCR的“优点”（原理类似），同时也照顾了因变量，这与回归的目的是吻合的。
