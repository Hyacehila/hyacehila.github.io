---
layout: blog-post
title: The Statistical Crisis in Science
date: 2026-01-24 10:00:00 +0800
categories: [统计学]
tags: [Hypothesis Testing, P-Value, P-Hacking]
author: Hyacehila
excerpt: 假设检验很强大，但它依赖前提；p 值显著也不等于“结论正确”的概；任何不精通统计学的人都可能在无意中进行统计造假。
---

# The Statistical Crisis in Science

> 本文主要观点整理自 Andrew Gelman 的一次讲座，即The Statistical Crisis in Science，部分内容来自Kamoun, S. (2022). Death by Statistics. Zenodo以及一些自己的思考。

统计学是一门研究数据的科学：我们希望“数据会说话”，告诉我们它们代表着什么。数据可能真的会说话，但我们无法直接理解，而统计学就是那个翻译官。

现代科学几乎处处使用统计学：分析生化实验、观察临床数据、研究大选结果……数据无处不在，统计学也无处不在。统计学家钻研方法并分享结论，供其他领域的研究者使用。

若论统计学中最重要的发明，**假设检验（hypothesis testing）** 很可能当之无愧：它体现了统计学的核心要义——**对不确定性的精确度量**。在近百年的统计学研究中，假设检验一直是最为活跃的领域之一。对许多应用研究者而言，“相信检验结果”似乎是个不错的主意，很多时候也确实能带来可操作的决策。

遗憾的是，这也构成了现代科学的一类统计危机：在现实研究里，**不满足前提的检验**、**多重比较**与**选择性报告**，会让假设检验给出看似有力、却可能荒谬的结果；人们有时甚至会更信任一串数字，而忽视了自己多年积累的领域经验。

## 假设检验强大，但它依赖前提

统计推断依赖一系列精妙假设：例如独立同分布（i.i.d.）抽样、随机抽样、噪声结构可控、模型设定合理等。但在大数据时代，**非随机抽样（not random sample）** 极其普遍；数据也往往掺杂复杂噪声、偏差与选择机制。

在这些数据上直接套用假设检验，得到的“显著/不显著”结论可能会误导。假设检验可以给出一个非常强力的结论，但也不要忘记：**它的适用条件是否在本次研究中成立**，很大程度上取决于研究者的领域知识与研究设计质量。

统计学或者说已经成熟的统计工具频繁地将研究人员引入歧途，科学家们盲目地遵循着这一套工具，缺乏批判性思考，但没有任何统计方法可以完全保护我们免受错误结果的侵害，也没有任何方法可以替代清晰的思考。人们不能期望统计学来完成人类归纳推理的工作，只有同时对统计工具与领域知识有着深入的了解才能完成它。

## p-value 与 p-hacking

p-value告诉你：**在零假设为真时**，观察到“和当前结果一样极端或更极端”的概率有多大。它衡量的是“这种数据在零假设下有多不寻常”，而不是“结论为真的概率”。

在一次孤立、设计良好的检验中，p-value低于 0.05 也许值得进一步关注；但如果你开始进行大量比较、尝试很多模型或很多指标，那么你会“理所应当”地因为偶然性看到一些很低的 p-value。更糟的是：即使完全没有任何有趣的真实效应，只要研究足够多，也会因为数量效应出现一些“显著”结果。

如果研究者有意无意地做了以下事情，就容易出现 **p-hacking**（为了得到可发表的“显著”结果而反复试探分析路径）：

- 反复尝试不同的变量、不同的特征构造、不同的分组方式
- 不断更换模型、损失函数、阈值、停止规则
- 只报告“最显著”的那一组结果，而忽略未显著的尝试

在 **Publish or Perish** 的压力下，p-hacking 变得诱人且常见，但它会系统性地制造虚假结论，侵蚀研究可信度。这种现象目前普遍存在，更多的集中在各领域的实证研究中，并且作者和期刊都对这些可疑做法持默许态度。

这里简单补充一句对初学者和非统计学者可能要注意的一点，当p-value小于阈值的时候，我们去拒绝原假设。一个自然的想法就是当 **p-value大于阈值的时候你会选择接受原假设，但这是错误的。** Fisher把比较大的 P 值（代表没有找到显著性证据）解释为：根据该组数据不能做出充分的判断。我们在此引用其原话：

> “相信一个假设已经被证明是真的，仅仅是由于该假设与已知的事实没有发生相互矛盾，这种逻辑上的误解，在统计推断上是缺乏坚实根基的，在其它类型的科学推理中也是如此。
> 当显著性检验被准确使用时，只要显著性检验与数据相矛盾，这个显著性检验就能够拒绝或否定这些假设，但该显著性检验永远不能确认这些假设一定是真的，……”

所以假设检验的目的在于试图找到证据拒绝原假设，而不在于证明什么是正确的。当没有足够证据拒绝原假设时，不采用 “接受原假设” 的表述，而采用 “不拒绝原假设” 的表述。“不拒绝”的表述实际上意味着并未给出明确的结论，我们没有说原假设正确，也没有说它不正确。 **总之，假设检验的主要目的是为了拒绝而不是接受。**

关于统计分布的假设检验，Yihui以前写过一篇博客，和这里想讨论的事情很类似，其核心结论可以引述为

> 根据数据检验总体的分布看来几乎没有什么用处，若拒绝零假设，即数据不服从某种分布，那么往往会使得下面要做的工作的前提假设不成立——这显然会很惨；
> 若不拒绝零假设——这几乎是无用的结论，因为不拒绝这个零假设，不代表能拒绝其它零假设，因此你仍然不知道数据是什么分布——这显然更惨；
> 所以我们要把自己的眼睛捂上，假装看不见，像数理统计学家那样，我们假定X服从帕累托分布。

Yihui的文风确实很有特色，[原文参考](https://yihui.org/cn/2009/02/test-statistical-distributions/) 

对于 p-hacking 或许可以看看这个图，完全依靠重复实验的运气去获得所谓的显著，图来自统计之都

![p-hacking](https://raw.githubusercontent.com/tcya/tcya.github.io/master/assets/images/xkcd_significant.jpg)

## 关于可视化
可视化也是统计学一个非常重要的组成部分，从某种意义上讲它是文章最重要的部分，一个美观的图表是读者获取你想表达的内容的最简单方式，而不是大段的文字叙述。遗憾的是在可视化领域的问题不比假设检验更少。Cleveland 在几十年前就讨论图形的各种缺陷，近10年以来学术界对于Barplot的批评也层出不穷，但顶级期刊上依旧遍布质量极差的 Barplot。研究者用它们掩盖实验数据的缺陷，欺骗审稿人与编辑以获得发表机会。

## An Example：经济学中的自动变量选择
请在阅读本节后面的内容的时候不断思考这句话：**我们是在用数据检验理论，还是在用数据构建理论来迎合我们的偏见？**

在机器学习（ML）中，自动特征选择（如 LASSO, Stepwise, Random Forest importance）非常流行，因为目标是预测精度。只要在测试集上表现好，变量是怎么选出来的并不重要。

但在经济学中，目标通常是因果推断（Causal Inference）。我们需要看特定变量（比如政策冲击）的系数$\beta$的显著性。如果我们使用统计软件自带的逐步回归（Stepwise Regression），或者手动尝试各种控制变量的组合，剔除那些"不显著"的变量，保留那些"显著"的。这种看起来是在"清洗模型"，但这往往属于 P-hacking 的灰色地带，甚至就是直接的作弊。Edward Leamer 早在 1983 年的文章《Let's Take the Con Out of Econometrics》中就讨论了这个问题，但是几十年后的现在它依旧在所有实证研究领域泛滥。

标准的 P 值计算假设你的模型是在看数据之前就定好的。如果你先用数据挑选出"最好的"变量，再在同一个数据上计算 P 值，那么这个 P 值是完全失效的。因为你已经消耗了数据的自由度来筛选模型，但并没有在计算中对此进行惩罚。这会导致标准误被严重低估，P 值被人为压低。变量选择后的推断，在经济学场景中是失效的。

即使研究者没有主观恶意的 P-hacking，由于数据处理过程中存在无数个微小的选择（如：是否加入这个控制变量？是否取对数？），这些选择如果依赖于“是否能得到显著结果”，那么最终呈现的统计显著性就是一种幻觉。

对于普通研究者而言，最诚实的做法不是依赖自动筛选出一个“完美模型”，而是进行 敏感性分析：展示你的结论在不同控制变量组合下是否依然稳健，而不是只展示那个 P < 0.05 的结果。但**Publish or Perish**的压力悬浮在几乎任何研究者的身上，诚实可能并没有任何价值。

## 更稳健的使用建议
区分“探索性研究”与“验证性研究” (Preregistration)；对于验证性研究，在看到数据之前，你就已经明确了假设、模型设定、变量定义和样本量计算。这种情况下 P 值是有意义的。而对于探索性研究，拿到数据后，试图从中寻找规律。这完全没问题，甚至对科学发现至关重要。

关注效应量与置信区间，而非仅仅是 P 值；告诉读者"影响有多大"。是收入增加了 1%，还是增加了 50%，使用置信区间而不是单纯的点估计，它包含着更多的信息。特别地，使用散点图来代替条形图等模糊结构，清晰地告诉读者每个点的位置再说明整体分布情况，而不是把精确数据隐藏在模糊的图形之后。

进行敏感性分析，不要只展示那个 P 值最小的模型，尝试改变控制变量的组合、改变数据的清洗方式、改变回归的函数形式（线性 vs 对数）。如果核心结论在 80% 的模型设定下都保持稳定，那么你的结论就是稳健的；如果结论仅仅依赖于某几个特定的控制变量，那么它很可能只是噪音。这在近期文献中被称为检验“效应的振动”（Vibration of Effects）。







