---
layout: blog-post
title: "Re0-03 : HuggingFace TRL DPOTrainer"
date: 2025-12-29 22:00:00 +0800
series: "Re0 : 从0开始的LLM Finetuning"
categories: [Finetuning]
tags: [RL, DPO]
author: Hyacehila
excerpt: 详解使用 TRL DPOTrainer微调LLM：实现简单的偏好对齐
---

# 第三章：使用 DPO 进行偏好对齐

> 配套代码：`dpotrainer.py`

## 前言：从指令遵循到偏好对齐

在前两章中，我们通过 SFT（监督微调）让模型学会了“如何回答问题”。然而，SFT 仅仅教会了模型模仿训练数据的分布，却无法区分回答的质量优劣。SFT 的 Loss 计算对所有样本一视同仁，无法反映“哪个回答更好”。

为了让模型向人类偏好的方向对齐，我们需要引入 **RLHF（Reinforcement Learning from Human Feedback）**。而 **DPO（Direct Preference Optimization）** 作为一种无需显式奖励模型的优化方法，通过巧妙的数学变换，将强化学习问题转化为了一种特殊的监督学习问题。本章将深入推导 DPO 的理论基础，并介绍如何使用 TRL 进行实践。

## 1. DPO 的理论推导

要理解 DPO 为什么有效，我们需要回到 RLHF 的数学本质。DPO 的核心贡献在于它通过数学推导证明了：**最优策略本身就隐含了奖励函数**。

### 1.1 RLHF 的原始优化目标

在标准的 RLHF 流程中，我们的目标是最大化期望奖励，同时添加一个 KL 散度约束，防止模型偏离参考模型（Reference Model）太远。其目标函数如下：

$$
\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(\cdot|x)} [r(x, y)] - \beta \mathbb{D}_{KL}(\pi(\cdot|x) || \pi_{\text{ref}}(\cdot|x))
$$

其中：
*   $r(x, y)$ 是训练好的奖励模型。
*   $\pi_{\text{ref}}$ 是 SFT 后的参考模型。
*   $\beta$ 是控制 KL 惩罚强度的超参数。

### 1.2 最优策略的解析解

这是一个典型的受约束优化问题。根据变分法（Calculus of Variations），我们可以求得该目标函数的**最优策略（Optimal Policy）** $\pi^*$ 的解析形式：

$$
\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right)
$$

其中 $Z(x) = \sum_y \pi_{\text{ref}}(y|x) \exp(\frac{1}{\beta} r(x, y))$ 是归一化常数（配分函数）。

### 1.3 奖励函数的逆向表达

DPO 的神来之笔在于：既然最优策略 $\pi^*$ 和奖励函数 $r(x,y)$ 存在上述一一对应关系，我们完全可以将上述公式反转，用策略模型来表示奖励函数。

对等式两边取对数并移项：

$$
\begin{aligned}
\log \pi^*(y|x) &= \log \pi_{\text{ref}}(y|x) + \frac{1}{\beta} r(x, y) - \log Z(x) \\
\frac{1}{\beta} r(x, y) &= \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \log Z(x) \\
r(x, y) &= \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)
\end{aligned}
$$

这个公式揭示了一个深刻的结论：**奖励值本质上就是策略模型相对于参考模型的对数概率比（Log-likelihood Ratio）**。

### 1.4 DPO 的最终损失函数

在偏好数据集中，我们希望模型对“优胜回答” $y_w$ 的奖励大于“失败回答” $y_l$ 的奖励。基于 Bradley-Terry 偏好模型，人类选择 $y_w$ 优于 $y_l$ 的概率为：

$$
P(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l))
$$

将步骤 1.3 推导出的 $r(x, y)$ 代入上式（注意 $Z(x)$ 在相减时被消去）：

$$
\begin{aligned}
r(x, y_w) - r(x, y_l) &= \beta \log \frac{\pi(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \\
&= \beta \log \left( \frac{\pi(y_w|x)}{\pi_{\text{ref}}(y_w|x)} \cdot \frac{\pi_{\text{ref}}(y_l|x)}{\pi(y_l|x)} \right)
\end{aligned}
$$

最终，我们将目标设定为最大化上述偏好概率的对数似然，得到 DPO 的损失函数：

$$
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
$$

**物理含义**：我们不需要显式的奖励模型 $r(x,y)$。只要优化这个损失函数，就是在隐式地优化原 RLHF 目标。它迫使策略模型 $\pi_\theta$ 增加 $y_w$ 的相对概率，同时降低 $y_l$ 的相对概率。

## 2. 偏好数据集构建

DPO 训练依赖于三元组格式的数据：`(Prompt, Chosen, Rejected)`。

```json
{
    "prompt": "如何解释量子纠缠？",
    "chosen": "量子纠缠是量子力学中的一种现象，描述了两个或多个粒子...", 
    "rejected": "不知道，量子力学太难了。"
}
```

常用的开源偏好数据集包括：
*   **`trl-lib/ultrafeedback_binarized`**：包含细粒度指令遵循的偏好数据。
*   **`Anthropic/hh-rlhf`**：关注有用性（Helpfulness）和无害性（Harmlessness）。

## 3. DPOTrainer 工程实践

使用 TRL 库的 `DPOTrainer` 可以快速实现上述逻辑。

### 3.1 核心配置：DPOConfig

```python
from trl import DPOConfig

training_args = DPOConfig(
    output_dir="./dpo-output",
    beta=0.1,                # 即公式中的 beta，控制 KL 约束强度
    loss_type="sigmoid",     # 使用标准的 sigmoid 损失
    learning_rate=5e-7,      # DPO 通常使用极低的学习率
    max_length=2048,
    gradient_checkpointing=True,
    bf16=True
)
```

**参数解析**：
*   **`beta`**：对应理论公式中的 $\beta$。
    *   **较小 (0.05)**：KL 惩罚弱，允许模型大幅偏离参考模型，适合数据质量极高且确定性强的场景。
    *   **较大 (0.5)**：KL 惩罚强，强迫模型保留更多原始分布，适合噪声较大的数据。
*   **`learning_rate`**：通常比 SFT 低一个数量级（如 `5e-7`），以避免破坏预训练的语言能力。

### 3.2 参考模型（Reference Model）的处理

DPOTrainer 需要参考模型来计算分母 $\pi_{\text{ref}}$。在工程实现上，有以下几种策略：

1.  **自动创建（推荐）**：设置 `ref_model=None`。Trainer 会自动复制一份策略模型并冻结参数。如果使用 PEFT (LoRA)，Trainer 会利用适配器的禁用/启用机制来复用同一个基础模型，即：
    *   计算 $\pi_\theta$ 时：启用 LoRA Adapter。
    *   计算 $\pi_{\text{ref}}$ 时：禁用 LoRA Adapter。
    *   这种方式显著节省显存。
2.  **显式加载**：手动加载一个 `AutoModelForCausalLM` 作为参考模型。

```python
trainer = DPOTrainer(
    model=model,            # 策略模型（带 LoRA）
    ref_model=None,         # 自动复用基础模型作为参考
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
    peft_config=lora_config
)
```

## 4. 训练监控与评估

DPO 的训练过程不像 SFT 那样只需关注 Loss 下降，我们需要监控以下特定的指标来判断对齐效果：

*   **`rewards/margins`**：表示 $r(x, y_w) - r(x, y_l)$ 的平均值。
    根据公式，这等价于 $\beta (\log \frac{\pi_\theta(y_w)}{\pi_{\text{ref}}(y_w)} - \log \frac{\pi_\theta(y_l)}{\pi_{\text{ref}}(y_l)})$。该值稳步上升意味着模型越来越能区分好坏。
*   **`rewards/accuracies`**：表示模型判断正确的比例（即给 Chosen 的分高于 Rejected 的比例）。训练良好的模型该值应接近 1.0。
*   **`objective/kl`**：策略模型与参考模型的 KL 散度 $\mathbb{D}_{KL}(\pi_\theta || \pi_{\text{ref}})$。
    *   若 KL 爆发式增长，说明 `beta` 太小或学习率太高。
    *   若 KL 接近 0，说明模型没有任何学习。

## 5. DPO 的局限性

尽管 DPO 极大地简化了偏好对齐流程，但它仍存在局限：

1.  **离线性质 (Off-policy)**：DPO 仅在给定的静态偏好数据上训练。公式中的期望 $\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}$ 是基于固定的数据集分布，而不是模型当前的采样分布。
2.  **对数据质量敏感**：如果偏好数据中的 `chosen` 实际上质量平庸，或者 `chosen` 与 `rejected` 差异不明显，DPO 的效果会大打折扣。
3.  **缺乏真值反馈**：对于数学、代码等有客观标准（Ground Truth）的任务，DPO 这种基于比较的相对学习效率不如基于结果的绝对学习。

针对第 3 点，如果你需要处理可验证的客观任务（如“让模型做对数学题”），我们在下一章介绍的 **GRPO** 将是更好的选择。

---

## 参考资料

- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al., 2023)](https://arxiv.org/abs/2305.18290)
- [Hugging Face TRL: DPOTrainer Documentation](https://huggingface.co/docs/trl/main/en/dpo_trainer)
