---
layout: blog-post
title: 从欧氏空间到流形拓扑：高维数据的降维之旅
date: 2026-01-16 10:00:00 +0800
categories: [机器学习]
tags: [降维, 流形学习, Learning]
author: Hyacehila
excerpt: 深入剖析从线性保持(PCA/MDS)到非线性流形(t-SNE/UMAP)的降维技术演进,探讨这些方法背后的数学直觉与技术细节
---

# 从欧氏空间到流形拓扑：高维数据的降维之旅

在现代数据科学与机器学习研究中，我们往往不得不面对成百上千甚至更高维度的数据。为了理解这些数据，**降维（Dimensionality Reduction）** 成为了必不可少的手段。这不仅仅是为了数据压缩，更是为了克服“维数灾难”，发掘数据背后的潜在结构。

本文将从最朴素的近邻思想出发，沿着从**线性保持**（PCA/MDS）到**非线性流形**（t-SNE/UMAP）的演进路线，深入剖析这些方法背后的数学直觉与技术细节。

本章我们不详细的讨论整个降维技术，而是侧重其中最实用的几种，作为应用层面的介绍。对于已经在其他笔记中详细介绍过的内容（如PCA，AE等），这里仅仅进行简单的介绍，不再详细关注其理论。

## 降维的动机与背景

### 起源：k近邻学习（kNN）

一切降维的初衷，往往都可以回溯到最简单的**k近邻（k-Nearest Neighbor, kNN）** 思想。kNN 是一种"懒惰学习"（lazy learning）的代表：它没有显式的训练过程，而是将训练样本保存起来。

其**基本原理**是：给定测试样本，基于某种距离度量找出训练集中与其最靠近的 $k$ 个训练样本，利用这些邻居的信息（投票或加权平均）进行预测。kNN 的成功高度依赖于两个因素：**合适的 $k$ 值**选取，以及**有效的距离度量**。

### 困境：维数灾难（The Curse of Dimensionality）

kNN 的有效性基于一个隐式假设：**采样密度足够大**。然而，在高维空间中，这个假设瞬间崩塌。

这主要表现为两个方面的问题。首先是**稀疏性**：随着维数增长，想要维持相同的采样密度，所需样本量呈指数级爆炸。其次是**距离失效**：在高维空间中，点与点之间的欧氏距离倾向于变得均匀（即最近邻和最远邻的距离差异变小），导致"距离"这一概念失去了区分度。

这个问题不仅仅影响kNN这种简单的机器学习算法，对于几乎所有算法都会造成极大的影响。为了缓解这一问题，我们将目光投向了**低维嵌入**：寻找一个低维子空间，使样本密度相对提高，距离计算重新变得有意义。

## 保持全局结构的传统方法

早期的降维技术主要关注如何保持数据点之间的**全局几何关系**（如欧氏距离）。

### 多维缩放（MDS）：距离的忠实还原

MDS（Multiple Dimensional Scaling）是保留距离的经典算法。它的核心直觉非常朴素：**如果两个点在高维空间距离很远，在低维空间也应该很远。**

假定 $m$ 个样本在原始空间的距离矩阵为 $D$，分量为 $dist_{ij}$。我们的目标是找到低维映射 $Z \in \mathbb{R}^{d^* \times m}$，使得 $\|z_{i}-z_{j}\| \approx dist_{ij}$。

数学上，通过构建内积矩阵 $B = Z^{\mathrm{T}} Z$，利用余弦定理推导：
$$b_{ij}=-\frac{1}{2}(dist_{ij}^{2}-dist_{i.}^{2}-dist_{.j}^{2}+dist_{..}^{2})$$
其中 $dist_{i.}^2$ 等表示行或列的均值。对矩阵 $B$ 进行特征值分解：
$$Z = \Lambda_{*}^{1/2} V_{*}^{\mathrm{T}}$$
取最大的几个特征值对应的特征向量，即可得到坐标。

MDS 的主要局限在于：它严格试图保留所有点对之间的距离，这种近乎严苛的对欧式距离（也可能是其他距离）的要求很可能会产生错误。在处理非线性数据（如卷曲的瑞士卷）时，欧氏距离本身就是错误的度量（两个卷曲面上空间距离近的点，在流形上实际很远）。

### PCA 与 KPCA：从线性到核技巧

**PCA（主成分分析）** 已经有了很详细的介绍，我们这里仅仅把它放在整个降维的宏观视角中再简单讨论。但值得重申的是，PCA 等价于**使用欧氏距离的 MDS**。它寻找最大方差方向，本质上是保留数据的**全局线性结构**。

当线性投影失效时，**核化线性降维（KPCA）**引入了"核技巧"（Kernel Trick）。其**直觉**是：将低维线性不可分的数据映射到高维（甚至无穷维）空间，使其变得线性可分。

在**形式**上，通过求解 $\left(\sum_{i=1}^m z_i z_i^\mathrm{T}\right) W = \lambda W$ 并在计算中利用核函数 $\kappa(x_i, x_j)$ 替代直接的内积计算。虽然把数据映射到更高维看似反直觉，但它为 PCA 引入了**非线性处理能力**，是连接传统统计方法与流形学习的桥梁。

## 流形学习与概率图模型（现代降维核心）

当数据分布在复杂的曲面（流形）上时，我们需要**流形学习（Manifold Learning）**。其核心思想是：**关注局部邻域结构，忽略遥远的全局距离。** 至于流形本身是什么，在拓扑学中有更为详细的讨论。

### t-SNE：从距离到概率分布

**t-SNE (t-Distributed Stochastic Neighbor Embedding)** 是深度学习时代前夜最重要的数据可视化技术。它彻底抛弃了“保持距离”的硬性约束，转而追求“保持概率分布”。

#### 技术直觉与数学形式

**高维空间的邻居概率（高斯分布）**：在高维空间，如果不直接用距离，而是问：点 $x_j$ 是点 $x_i$ 的邻居的概率是多少？我们使用**高斯分布**来定义这个条件概率 $p_{j|i}$
$$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}$$
注意这里的 $\sigma_i$ 是针对每个点单独计算的，由参数 **Perplexity** 决定。

**低维空间的邻居概率（t-分布）**：在低维空间，我们要寻找点 $y_i, y_j$。为了解决**拥挤问题（Crowding Problem）**，t-SNE 使用自由度为 1 的 **t-分布（柯西分布）**
$$q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}$$

**为什么要用 t-分布？** 因为 t-分布是"重尾"的。相比高斯分布，要获得相同的概率值（相似度），t-分布要求点与点之间的距离**更远**。这迫使原本在高维空间挤在一起的数据簇，在低维空间被"炸开"，形成清晰分离的簇。

**损失函数：KL 散度**
$$C = KL(P||Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}$$
从**梯度分析**来看：KL 散度是不对称的。如果 $p_{ij}$ 很大（高维是邻居）但 $q_{ij}$ 很小（低维分开了），惩罚巨大。反之，如果 $p_{ij}$ 很小（高维不是邻居）但 $q_{ij}$ 很大，惩罚较小。最终**结果**是：t-SNE 极度擅长**保留局部结构**（把邻居聚在一起），但几乎不保留全局结构（簇与簇之间的距离通常没有意义）。

非常重要的,  t-SNE 是一个相当低效率的方法，其在大规模数据上的低速度也是我们引入UMAP的核心原因之一。

**Perplexity (困惑度)**：这是t-SNE的核心参数 , 常取值范围在 5-50 之间，可以理解为"预估的邻居数量"。如果设置得太小，数据会碎裂成无数小团块，以此拟合噪音；如果设置得太大，则会忽略局部细节，结果越来越像 PCA。

### UMAP：拓扑数据分析的胜利

**UMAP (Uniform Manifold Approximation and Projection)** 是目前的 SOTA 降维算法。它在 t-SNE 的基础上，引入了严谨的黎曼几何与代数拓扑理论，解决了 t-SNE 速度慢且丢失全局结构的问题。

#### 技术直觉：流形上的均匀分布

UMAP 基于一个假设：**数据均匀分布在某个黎曼流形上。** 如果在现实空间中数据看起来分布不均（有的地方密，有的地方疏），那是因为我们用来观测的"标尺"（欧氏距离）是错的。

**自适应的度量（黎曼度量）**：UMAP 为每个点 $x_i$ 定义了一个局部的黎曼度量。简单来说，在数据稀疏的区域，UMAP 会"拉长"距离；在密集的区域，会"缩短"距离。这通过 k-近邻距离来实现，构建了一个加权的 kNN 图。

**模糊单纯复形（Fuzzy Simplicial Complex）**：通过上述度量，UMAP 将数据转化为拓扑结构（单纯复形）。

**优化目标：二元交叉熵 (Binary Cross-Entropy)**：t-SNE 只用了 KL 散度，只关注"把邻居拉近"。UMAP 使用交叉熵：
$$CE = \sum p_{ij} \log(\frac{p_{ij}}{q_{ij}}) + \sum (1-p_{ij}) \log(\frac{1-p_{ij}}{1-q_{ij}})$$
其中**第一项**类似 t-SNE，产生引力，拉近邻居。第二项涉及 $(1-p_{ij})$，这是在惩罚"非邻居被拉近"的情况，产生了一种**全局斥力**。最终**结果**是：UMAP 不仅让簇内紧凑，还迫使**簇与簇之间保持正确的相对位置**，从而保留了更多的全局结构。

UMAP的关键参数比t-SNE多,除了去控制局部的规模以外,我们还需要去控制嵌入的紧凑程度,因为他增加了研究全局结构的功能,可以展示一定程度的空间拓扑结构。

**`n_neighbors`**：控制局部图的规模。小值（例如 5）可以捕捉高频细节；大值（例如 200）则捕捉全局概貌（类似 PCA）。

**`min_dist`**：控制低维嵌入的紧凑度。小值（0.1）允许点重叠，适合聚类分析；大值（0.8）强迫点分开，适合展示拓扑结构。

## 神经网络与现代扩展

### Autoencoder (AE)：非线性压缩

对于AE及其衍生本身是什么，我们在自编码器章节中已经进行详细的讨论了。但从降维视角看，AE 是**参数化**的降维。它的主要**区别**在于：流形方法是非参数的（Non-parametric），它们只给出坐标，无法直接处理新数据；而 AE 学习的是一个函数 $f(x)$，可以随时处理新样本。

普通 AE 用于降维效果一般，对于t-SNE和UMAP这种经常将空间压缩到2-3维的场景，AE只适合降维到几十或者几百作为特征预处理的一种方法。将提取的特征交给下游任务使用，如 **VAE (Variational AE)** 及其变体在生成模型(Stable Disfusion)和特征解耦中是核心技术。

### 现代科研中的其他重要模型（值得关注）

在现代生物信息学（特别是单细胞测序）和计算机视觉研究中，除了 UMAP，还有两个值得注意的模型：

**PHATE (Potential of Heat-diffusion for Affinity-based Transition Embedding)**：其**直觉**是使用"热扩散"过程来模拟数据点之间的转移概率。主要**优势**在于：它特别擅长保留数据的**轨迹结构（Trajectory）**和分支结构（如干细胞分化过程），比 UMAP 能更好地展示数据的连续演化过程。

**PacMAP (Pairwise Controlled Manifold Approximation)**：其**直觉**是通过设计特殊的"中距离"点对，显式地平衡局部引力和全局斥力。目前的**地位**是：被认为是 t-SNE 和 UMAP 的有力竞争者，往往在保留全局结构上比 UMAP 更稳健，且参数敏感性更低。

## 度量学习（Metric Learning）

最后，回到降维的初衷。我们之所以降维，往往是因为欧氏距离在高维空间失效。**度量学习**提出了一个逆向思维：**与其将数据映射到低维空间去适应欧氏距离，不如直接学习一个新的距离度量函数 $d(x_i, x_j)$。**

在**马氏距离学习**方面：学习一个矩阵 $M$，使得距离 $d(x, y) = \sqrt{(x-y)^T M (x-y)}$ 能最好地反映数据的相似性。

在**孪生神经网络 (Siamese Networks)** 方面：这是现代深度度量学习的主流。通过神经网络将两个输入映射到特征空间，直接优化特征向量之间的距离（如 Triplet Loss），使得同一类样本距离近，不同类样本距离远。

## 总结：如何选择？

| 方法 | 核心数学思想 | 对全局结构的保留 | 对局部簇的保留 | 推荐场景 |
| :--- | :--- | :--- | :--- | :--- |
| **PCA** | 协方差特征分解 | 🌟🌟🌟 (极好) | 🌟 (差) | 数据预处理、基线测试、线性数据 |
| **MDS** | 距离矩阵重构 | 🌟🌟🌟 | 🌟 | 需要严格保持距离矩阵时 |
| **t-SNE** | 概率分布 (KL散度) | 🌟 (几乎无) | 🌟🌟🌟 (极好) | 探索性数据分析、强调聚类分离度 |
| **UMAP** | 拓扑单纯复形 + 交叉熵 | 🌟🌟 (良好) | 🌟🌟🌟 (极好) | **目前首选**，兼顾全局与局部，大数据集 |
| **PHATE** | 热扩散信息距离 | 🌟🌟🌟 | 🌟🌟 | 具有连续演化轨迹的数据（如时间序列、生物发育） |
| **AE** | 神经网络重构误差 | 🌟🌟 | 🌟 | 需要特征提取器用于下游任务时 |